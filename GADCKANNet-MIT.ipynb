{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enabling Deterministic Behaviour "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "# Enable deterministic behavior\n",
    "import os\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "\n",
    "np.random.seed(65)\n",
    "random.seed(65)\n",
    "tf.random.set_seed(65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GADC-KANNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tfkan.layers import DenseKAN, Conv2DKAN\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, concatenate, Conv2DTranspose, BatchNormalization, Input, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.layers as KL\n",
    "import numpy as np\n",
    "\n",
    "# Dilated Residual Activation Path (DRAP) function\n",
    "def DRAP(x, filters, size):\n",
    "    outputs = Conv2D(kernel_size=size, filters=filters, strides=1, dilation_rate=(2, 2), padding='same', use_bias=False)(x)\n",
    "    outputs = BatchNormalization()(outputs)\n",
    "    x = Conv2D(kernel_size=(1, 1), filters=filters, strides=1, padding='same')(x)\n",
    "    outputs = KL.add([outputs, x])\n",
    "    outputs = Activation('relu')(outputs)\n",
    "    return outputs\n",
    "\n",
    "# Kolmogorov Arnold Network based Squeeze and Excitation (KAN-SE)\n",
    "def kan_se(x, r=4):\n",
    "    input_channel = x.shape[-1]\n",
    "    outputs = KL.GlobalMaxPooling2D()(x)\n",
    "    outputs = KL.Reshape((1, 1, input_channel))(outputs)\n",
    "    #outputs = KL.Dense(input_channel // r)(outputs)\n",
    "    outputs = DenseKAN(input_channel // r)(outputs)\n",
    "    print(input_channel/r)\n",
    "    #print(input_channel // r)\n",
    "    outputs = Activation('relu')(outputs)\n",
    "    #outputs = KL.Dense(input_channel)(outputs)\n",
    "    outputs = DenseKAN(input_channel)(outputs)\n",
    "    print(input_channel)\n",
    "    outputs = Activation('sigmoid')(outputs)\n",
    "    outputs = KL.Multiply()([x, outputs])\n",
    "    return outputs\n",
    "\n",
    "# Kolmogorov Arnold Network based Feature Selective Fusion (KAN-FSF)\n",
    "def kan_fsf(low_x, high_x):\n",
    "    outputs = concatenate([high_x, low_x], axis=3)\n",
    "    outputs = kan_se(outputs)\n",
    "    outputs = Conv2D(kernel_size=(1, 1), filters=int(outputs.shape[-1] / 2), strides=1, padding='same')(outputs)\n",
    "    outputs = KL.GlobalMaxPooling2D()(outputs)\n",
    "    outputs = KL.Reshape((1, 1, outputs.shape[-1]))(outputs)\n",
    "    outputs = Activation('sigmoid')(outputs)\n",
    "    outputs = KL.Multiply()([low_x, outputs])\n",
    "    outputs = KL.add([outputs, high_x])\n",
    "    return outputs\n",
    "\n",
    "#Gradient Aware Directional Convolution (GADC)\n",
    "class GADC(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, kernel_size=(3, 3), strides=(1, 1), padding='same', **kwargs):\n",
    "        super(GADC, self).__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "\n",
    "        # Define standard convolutions for all directions\n",
    "        self.horizontal_conv = Conv2D(filters, kernel_size=(1, kernel_size[1]), strides=strides, padding=padding)\n",
    "        self.vertical_conv = Conv2D(filters, kernel_size=(kernel_size[0], 1), strides=strides, padding=padding)\n",
    "        self.diagonal_conv = Conv2D(filters, kernel_size=kernel_size, strides=strides, padding=padding)\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        sobel_x = tf.image.sobel_edges(inputs)[..., 0]\n",
    "        sobel_y = tf.image.sobel_edges(inputs)[..., 1]\n",
    "        gradient_magnitude = tf.sqrt(tf.square(sobel_x) + tf.square(sobel_y))\n",
    "        gradient_direction = tf.atan2(sobel_y, sobel_x)\n",
    "        # Horizontal and vertical convolutions\n",
    "        horizontal_output = self.horizontal_conv(inputs)\n",
    "        vertical_output = self.vertical_conv(inputs)\n",
    "\n",
    "        # Rotate input for diagonal convolutions\n",
    "        rotated_45 = tf.image.rot90(inputs, k=1)  # Rotate 90 degrees counter-clockwise\n",
    "        diagonal_45_output = self.diagonal_conv(rotated_45)\n",
    "        diagonal_45_output = tf.image.rot90(diagonal_45_output, k=3)  # Rotate back 90 degrees clockwise\n",
    "\n",
    "        rotated_135 = tf.image.rot90(inputs, k=3)  # Rotate 90 degrees clockwise\n",
    "        diagonal_135_output = self.diagonal_conv(rotated_135)\n",
    "        diagonal_135_output = tf.image.rot90(diagonal_135_output, k=1)  # Rotate back 90 degrees counter-clockwise\n",
    "  \n",
    "      \n",
    "        weights_horizontal = tf.cast(\n",
    "            tf.logical_or(\n",
    "                tf.logical_or(\n",
    "                    tf.logical_and(gradient_direction >= -0.5, gradient_direction <= 0.5),\n",
    "                    tf.logical_and(gradient_direction >= 2.64, gradient_direction <= np.pi)\n",
    "                ),\n",
    "                tf.logical_and(gradient_direction >= -np.pi, gradient_direction <= -2.64)\n",
    "            ),\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "\n",
    "        # Vertical weights (orthogonal to horizontal)\n",
    "        weights_vertical = 1.0 - weights_horizontal\n",
    "\n",
    "        # Diagonal 45° weights\n",
    "        weights_diagonal_45 = tf.cast(\n",
    "            tf.logical_or(\n",
    "                tf.logical_and(gradient_direction >= 0.2854, gradient_direction <= 1.2854),\n",
    "                tf.logical_and(gradient_direction >= -2.8554, gradient_direction <= -1.8554)\n",
    "            ),\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "\n",
    "        # Diagonal 135° weights\n",
    "        weights_diagonal_135 = tf.cast(\n",
    "            tf.logical_or(\n",
    "                tf.logical_and(gradient_direction >= 1.8554, gradient_direction <= 2.8554),\n",
    "                tf.logical_and(gradient_direction >= -1.2854, gradient_direction <= -0.2854)\n",
    "            ),\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "\n",
    "        # Combine the directional outputs using weights\n",
    "        output = (\n",
    "            horizontal_output * weights_horizontal +\n",
    "            vertical_output * weights_vertical +\n",
    "            diagonal_45_output * weights_diagonal_45 +\n",
    "            diagonal_135_output * weights_diagonal_135\n",
    "        )\n",
    "         # Combine outputs\n",
    "        #output = horizontal_output + vertical_output + diagonal_45_output + diagonal_135_output\n",
    "\n",
    "        return output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(GADC, self).get_config()\n",
    "        config.update({\n",
    "            \"filters\": self.filters,\n",
    "            \"kernel_size\": self.kernel_size,\n",
    "            \"strides\": self.strides,\n",
    "            \"padding\": self.padding,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "# U-Net model with ar_path in skip connections\n",
    "def base_unet_with_snake_kan_fsf_ar_path(filters, output_channels, width=None, height=None, input_channels=1, conv_layers=3):\n",
    "    def conv2d_with_snake(layer_input, filters, conv_layers=3):\n",
    "        d = Conv2D(filters, kernel_size=(3, 3), strides=(1, 1), padding='same')(layer_input)\n",
    "        d = BatchNormalization()(d)\n",
    "        d = Activation('relu')(d)\n",
    "        \n",
    "        d = GADC(filters, kernel_size=(3, 3), strides=(1, 1), padding='same')(d)\n",
    "        d = BatchNormalization()(d)\n",
    "        d = Activation('relu')(d)\n",
    "        \n",
    "        for i in range(conv_layers - 2):\n",
    "            d = Conv2D(filters, kernel_size=(3, 3), strides=(1, 1), padding='same')(d)\n",
    "            d = BatchNormalization()(d)\n",
    "            d = Activation('relu')(d)\n",
    "\n",
    "        return d\n",
    "\n",
    "    def deconv2d(layer_input, filters):\n",
    "        u = Conv2DTranspose(filters, 2, strides=(2, 2), padding='same')(layer_input)\n",
    "        u = BatchNormalization()(u)\n",
    "        u = Activation('relu')(u)\n",
    "        return u\n",
    "\n",
    "    inputs = Input(shape=(width, height, input_channels))\n",
    "\n",
    "    conv1 = conv2d_with_snake(inputs, filters, conv_layers=conv_layers)\n",
    "    pool1 = MaxPooling2D((2, 2))(conv1)\n",
    "\n",
    "    conv2 = conv2d_with_snake(pool1, filters * 2, conv_layers=conv_layers)\n",
    "    pool2 = MaxPooling2D((2, 2))(conv2)\n",
    "\n",
    "    conv3 = conv2d_with_snake(pool2, filters * 4, conv_layers=conv_layers)\n",
    "    pool3 = MaxPooling2D((2, 2))(conv3)\n",
    "\n",
    "    conv5 = conv2d_with_snake(pool3, filters * 8, conv_layers=conv_layers)\n",
    "\n",
    "    up7 = deconv2d(conv5, filters * 4)\n",
    "    conv2_ar = DRAP(conv3, filters * 4, (3, 3))  # Apply ar_path on conv2\n",
    "    fsf7 = kan_fsf(conv2_ar, up7)\n",
    "    conv7 = conv2d_with_snake(fsf7, filters * 4, conv_layers=conv_layers)\n",
    "\n",
    "    up8 = deconv2d(conv7, filters *2)\n",
    "    conv1_ar = DRAP(conv2, filters*2, (3, 3))  # Apply ar_path on conv1\n",
    "    fsf8 = kan_fsf(conv1_ar, up8)\n",
    "    conv8 = conv2d_with_snake(fsf8, filters*2, conv_layers=conv_layers)\n",
    "\n",
    "    up9 = deconv2d(conv8, filters)\n",
    "    conv0_ar = DRAP(conv1, filters, (3, 3))  # Apply ar_path on conv1\n",
    "    fsf9 = kan_fsf(conv0_ar, up9)\n",
    "    conv9 = conv2d_with_snake(fsf9, filters, conv_layers=conv_layers)\n",
    "\n",
    "    outputs = Conv2D(output_channels, kernel_size=(1, 1), strides=(1, 1), activation='sigmoid')(conv9)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Example usage:\n",
    "filters = 16  # Base number of filters\n",
    "output_channels = 1  # For binary segmentation\n",
    "width, height = 1024, 1024  # Example input size\n",
    "model = base_unet_with_snake_kan_fsf_ar_path(filters, output_channels, width=width, height=height, input_channels=3,conv_layers=2)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "from tfkan.layers import DenseKAN\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "import segmentation_models as sm \n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_width = 1024\n",
    "desired_height = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = []\n",
    "\n",
    "for directory_path in glob.glob(\"massachusetts-roads-dataset/tiff/train/\"):\n",
    "    for img_path in sorted(glob.glob(os.path.join(directory_path, \"*.tiff\"))):\n",
    "        #print(img_path)\n",
    "        img = cv2.imread(img_path, 1)  \n",
    "        img = cv2.resize(img, (desired_width, desired_height))   \n",
    "        img = img / 255.0  \n",
    "        train_images.append(img)\n",
    "       \n",
    "train_images = np.array(train_images)\n",
    "print(train_images.shape)\n",
    "#plt.imshow(train_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_masks = [] \n",
    "for directory_path in glob.glob(\"massachusetts-roads-dataset/tiff/train_labels/\"):\n",
    "    for mask_path in sorted(glob.glob(os.path.join(directory_path, \"*.tif\"))):\n",
    "        #print(mask_path)\n",
    "        mask = cv2.imread(mask_path, 0)     \n",
    "        mask = cv2.resize(mask, (desired_width, desired_height))  \n",
    "        mask = (mask > 0)\n",
    "        #mask = (mask > 0).astype(np.uint8) \n",
    "        mask=mask.astype(np.float64)\n",
    "        train_masks.append(mask)\n",
    "          \n",
    "train_masks = np.array(train_masks)\n",
    "print(train_masks.shape)\n",
    "#plt.imshow(train_masks[0],cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_images = []\n",
    "for directory_path in glob.glob(\"massachusetts-roads-dataset/tiff/val/\"):\n",
    "    for img_path in sorted(glob.glob(os.path.join(directory_path, \"*.tiff\"))):\n",
    "        #print(img_path)\n",
    "        img = cv2.imread(img_path, 1)     \n",
    "        img = cv2.resize(img, (desired_width, desired_height))  \n",
    "        img = img / 255.0   \n",
    "        val_images.append(img)\n",
    "              \n",
    "val_images = np.array(val_images)\n",
    "print(val_images.shape)\n",
    "#plt.imshow(val_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_masks = [] \n",
    "for directory_path in glob.glob(\"massachusetts-roads-dataset/tiff/val_labels/\"):\n",
    "    for mask_path in sorted(glob.glob(os.path.join(directory_path, \"*.tif\"))):\n",
    "        #print(mask_path)\n",
    "        mask = cv2.imread(mask_path, 0)  \n",
    "        mask = cv2.resize(mask, (desired_width, desired_height))     \n",
    "        mask = (mask > 0)\n",
    "        mask=mask.astype(np.float64)\n",
    "        val_masks.append(mask)\n",
    "                \n",
    "val_masks = np.array(val_masks)\n",
    "print(val_masks.shape)\n",
    "#plt.imshow(val_masks[0],cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = []\n",
    "for directory_path in glob.glob(\"massachusetts-roads-dataset/tiff/test/\"):\n",
    "    for img_path in sorted(glob.glob(os.path.join(directory_path, \"*.tiff\"))):\n",
    "        #print(img_path)\n",
    "        img = cv2.imread(img_path, 1)     \n",
    "        img = cv2.resize(img, (desired_width, desired_height))   \n",
    "        img = img / 255.0  \n",
    "        test_images.append(img)\n",
    "            \n",
    "test_images = np.array(test_images)\n",
    "print(test_images.shape)\n",
    "#plt.imshow(test_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_masks = [] \n",
    "for directory_path in glob.glob(\"massachusetts-roads-dataset/tiff/test_labels/\"):\n",
    "    for mask_path in sorted(glob.glob(os.path.join(directory_path, \"*.tif\"))):\n",
    "        #print(mask_path)\n",
    "        mask = cv2.imread(mask_path, 0) \n",
    "        mask = cv2.resize(mask, (desired_width, desired_height)) \n",
    "        mask = (mask > 0)  \n",
    "        mask=mask.astype(np.float64)\n",
    "        test_masks.append(mask)\n",
    "                 \n",
    "test_masks = np.array(test_masks)\n",
    "print(test_masks.shape)\n",
    "#plt.imshow(test_masks[0],cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.keras.callbacks.ModelCheckpoint('models/improvedkanunet.h5', verbose=1, save_best_only=True)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_scheduler(epoch,lr):\n",
    "    decay_rate=1e-6\n",
    "    return lr-decay_rate\n",
    "lr_callback=LearningRateScheduler(lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices=tf.compat.v1.config.experimental.list_physical_devices('GPU')\n",
    "print('GPU is available' if len(physical_devices) > 0 else 'Not available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(), loss=sm.losses.dice_loss, metrics=[sm.metrics.iou_score, sm.metrics.f1_score,sm.metrics.precision,sm.metrics.recall])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "with tf.device(\"/GPU:0\"):\n",
    "    history = model.fit(train_images, train_masks, batch_size=4, epochs=epochs, validation_data=(val_images, val_masks), callbacks=[checkpoint,early_stopping,lr_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " custom_objects = {'GADC': GADC}\n",
    "\n",
    "model = load_model('models/improvedkanunet.h5', compile=False, custom_objects=custom_objects)\n",
    "\n",
    "# Compile the model with the appropriate loss function and metrics\n",
    "model.compile(optimizer=Adam(),\n",
    "              loss=sm.losses.dice_loss,\n",
    "              metrics=[sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5), \n",
    "                       'accuracy', Precision(), Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval = model.evaluate(test_images, test_masks)\n",
    "print('Test accuracy: ' + \"{:.2f}\".format(eval[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize 10 images in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_indices = random.sample(range(0, len(test_images)), 10)\n",
    "test_sample = test_images[random_indices]\n",
    "\n",
    "predictions = model.predict(test_sample)\n",
    "predictions = (predictions > 0.5).astype(np.uint8)\n",
    "\n",
    "fig, axes = plt.subplots(10, 3, figsize=(10, 3*10))\n",
    "\n",
    "# Iterate over the image and mask pairs and display them in subplots\n",
    "for i in range(len(test_sample)):\n",
    "\n",
    "    image = (test_sample[i] * 255).astype(np.uint8)\n",
    "    mask = predictions[i]\n",
    "    #print(mask.shape)\n",
    "    ground_truth = test_masks[random_indices][i] #* np.array([255, 255, 255]) # convert the forground into yellow color to achieve the desired aesthetic\n",
    "    overlay = image.copy()\n",
    "\n",
    "    mask = np.repeat(mask, 3, axis=2) # matching the size of the channel of the mask and the image to perform an overlay\n",
    "    #print(mask.shape)\n",
    "    inverted_mask = 1 - mask\n",
    "\n",
    "    yellow_mask = np.array([255, 255, 255]) * mask\n",
    "\n",
    "    # Apply the mask on the image\n",
    "    result = image * inverted_mask + yellow_mask\n",
    "    alpha = 0.2\n",
    "    predicted_overlay = cv2.addWeighted(overlay, alpha, result.astype(overlay.dtype), 1 - alpha, 0)\n",
    "\n",
    "    # Plot the image and mask in the corresponding subplot\n",
    "    axes[i, 0].imshow(image)\n",
    "    axes[i, 0].set_title('Original')\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    axes[i, 1].imshow(ground_truth)\n",
    "    axes[i, 1].set_title('Ground Truth')\n",
    "    axes[i, 1].axis('off')\n",
    "    \n",
    "    axes[i, 2].imshow(yellow_mask)\n",
    "    axes[i, 2].set_title('Predicted')\n",
    "    axes[i, 2].axis('off')\n",
    "    \n",
    "    \n",
    "    \n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "#plt.savefig('result.png', bbox_inches='tight')  # Save as PNG image\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize all images in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = test_images[:49]\n",
    "\n",
    "predictions = model.predict(test_sample)\n",
    "predictions = (predictions > 0.5).astype(np.uint8)\n",
    "\n",
    "# Create a 49x3 grid of images (49 samples, each with 3 images)\n",
    "fig, axes = plt.subplots(49, 3, figsize=(15, 49*3))  \n",
    "\n",
    "# Iterate over the image and mask pairs and display them in subplots\n",
    "for i in range(len(test_sample)):\n",
    "\n",
    "    image = (test_sample[i] * 255).astype(np.uint8)\n",
    "    mask = predictions[i]\n",
    "    ground_truth = test_masks[i]  # No need to use random indices anymore\n",
    "\n",
    "    overlay = image.copy()\n",
    "\n",
    "    mask = np.repeat(mask, 3, axis=2)  # Match the size of the channel of the mask and the image to perform an overlay\n",
    "    inverted_mask = 1 - mask\n",
    "    yellow_mask = np.array([255, 255, 255]) * mask\n",
    "\n",
    "    # Apply the mask on the image\n",
    "    result = image * inverted_mask + yellow_mask\n",
    "    alpha = 0.2\n",
    "    predicted_overlay = cv2.addWeighted(overlay, alpha, result.astype(overlay.dtype), 1 - alpha, 0)\n",
    "\n",
    "    # Plot the image and mask in the corresponding subplot\n",
    "    axes[i, 0].imshow(image)\n",
    "    axes[i, 0].set_title('Original')\n",
    "    axes[i, 0].axis('off')\n",
    "\n",
    "    axes[i, 1].imshow(ground_truth)\n",
    "    axes[i, 1].set_title('Ground Truth')\n",
    "    axes[i, 1].axis('off')\n",
    "\n",
    "    axes[i, 2].imshow(yellow_mask)\n",
    "    axes[i, 2].set_title('Predicted')\n",
    "    axes[i, 2].axis('off')\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "#plt.savefig('unetSalArFSF1024dice1mlp1kan.png', bbox_inches='tight')  # Save as PNG image\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
